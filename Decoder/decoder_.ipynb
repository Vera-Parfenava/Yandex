{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dst': '- Intriguing.', 'src': '◄▴◓◠▨ ◨▽◠▦◈◬◓▪▼◬▵'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Чтение данных из файлов\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_data('ml_trainings.alien_translation/train')  # Путь к файлу train\n",
    "val_data = load_data('ml_trainings.alien_translation/val')  # Путь к файлу val\n",
    "test_data = load_data('ml_trainings.alien_translation/test_no_reference')  # Путь к файлу test_no_reference\n",
    "\n",
    "# Пример структуры данных\n",
    "print(train_data[0])  # Пример одного элемента из train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: spacy in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (0.13.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (75.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/vera/miniconda3/envs/my_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m952.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['◄', '▴', '◓', '◠', '▨', ' ', '◨', '▽', '◠', '▦', '◈', '◬', '◓', '▪', '▼', '◬', '▵']\n",
      "['-', 'Intriguing', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Загрузка модели для токенизации английского\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Функция для токенизации\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Для вымышленного языка будем использовать токенизацию по символам\n",
    "def tokenize_conlang(text):\n",
    "    return list(text)  # Разбиение по символам для вымышленного языка\n",
    "\n",
    "# Токенизация данных\n",
    "train_src = [item['src'] for item in train_data]\n",
    "train_dst = [item['dst'] for item in train_data]\n",
    "\n",
    "# Применение токенизации\n",
    "train_src_tokens = [tokenize_conlang(src) for src in train_src]  # Вымышленный язык\n",
    "train_dst_tokens = [tokenize_en(dst) for dst in train_dst]  # Английский язык\n",
    "\n",
    "# Пример\n",
    "print(train_src_tokens[0])  # Пример токенизации исходного текста\n",
    "print(train_dst_tokens[0])  # Пример токенизации целевого текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря для src: 180\n",
      "Размер словаря для dst: 66590\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Функция для построения словаря\n",
    "def build_vocab(tokens):\n",
    "    counter = Counter()\n",
    "    for sentence in tokens:\n",
    "        counter.update(sentence)\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "    return vocab\n",
    "\n",
    "# Построение словарей\n",
    "src_vocab = build_vocab(train_src_tokens)\n",
    "dst_vocab = build_vocab(train_dst_tokens)\n",
    "\n",
    "# Добавление специальных токенов\n",
    "src_vocab['<unk>'] = len(src_vocab)\n",
    "src_vocab['<pad>'] = len(src_vocab) + 1\n",
    "src_vocab['<sos>'] = len(src_vocab) + 2\n",
    "src_vocab['<eos>'] = len(src_vocab) + 3\n",
    "\n",
    "dst_vocab['<unk>'] = len(dst_vocab)\n",
    "dst_vocab['<pad>'] = len(dst_vocab) + 1\n",
    "dst_vocab['<sos>'] = len(dst_vocab) + 2\n",
    "dst_vocab['<eos>'] = len(dst_vocab) + 3\n",
    "\n",
    "# Обратные словари\n",
    "src_rev_vocab = {v: k for k, v in src_vocab.items()}\n",
    "dst_rev_vocab = {v: k for k, v in dst_vocab.items()}\n",
    "\n",
    "# Размеры словарей\n",
    "print(f\"Размер словаря для src: {len(src_vocab)}\")\n",
    "print(f\"Размер словаря для dst: {len(dst_vocab)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 2, 3, 4, 5, 6, 7, 3, 8, 9, 10, 2, 11, 12, 10, 13], [0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Функция для преобразования текста в последовательность индексов\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
    "\n",
    "# Преобразуем все данные в последовательности индексов\n",
    "train_sequences = [(text_to_sequence(src, src_vocab), text_to_sequence(dst, dst_vocab)) for src, dst in zip(train_src_tokens, train_dst_tokens)]\n",
    "\n",
    "# Пример преобразования\n",
    "print(train_sequences[0])  # Пример преобразования в индексы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([321, 300000])\n",
      "torch.Size([83, 300000])\n"
     ]
    }
   ],
   "source": [
    "# Функция для паддинга последовательностей\n",
    "def pad_sequences(batch, pad_token):\n",
    "    src_seqs, dst_seqs = zip(*batch)  # Разделяем src и dst\n",
    "    src_seqs_padded = pad_sequence([torch.tensor(seq) for seq in src_seqs], padding_value=pad_token)\n",
    "    dst_seqs_padded = pad_sequence([torch.tensor(seq) for seq in dst_seqs], padding_value=pad_token)\n",
    "    return src_seqs_padded, dst_seqs_padded\n",
    "\n",
    "# Пример паддинга\n",
    "src_pad_token = src_vocab['<pad>']\n",
    "dst_pad_token = dst_vocab['<pad>']\n",
    "\n",
    "# Паддинг для обучающего набора\n",
    "src_padded, dst_padded = pad_sequences(train_sequences, src_pad_token)\n",
    "\n",
    "print(src_padded.shape)  # Размер после паддинга для src\n",
    "print(dst_padded.shape)  # Размер после паддинга для dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 321]) torch.Size([64, 83])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Кастомный датасет\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, dst_data):\n",
    "        self.src_data = src_data\n",
    "        self.dst_data = dst_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.dst_data[idx]\n",
    "\n",
    "# Создание DataLoader\n",
    "train_dataset = TranslationDataset(src_padded.t(), dst_padded.t())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Пример батча\n",
    "for src_batch, dst_batch in train_loader:\n",
    "    print(src_batch.shape, dst_batch.shape)\n",
    "    break  # Пример одного батча\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.src_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.dst_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "    def forward(self, src, dst):\n",
    "        # Преобразуем входы в эмбеддинги\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        dst_embedded = self.dst_embedding(dst)\n",
    "\n",
    "        # Проходим через энкодер\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src_embedded)\n",
    "        \n",
    "        # Проходим через декодер\n",
    "        decoder_outputs, _ = self.decoder(dst_embedded, (hidden, cell))\n",
    "\n",
    "        # Выход из декодера\n",
    "        output = self.fc_out(decoder_outputs)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 66590])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Гиперпараметры\n",
    "input_dim = len(src_vocab)\n",
    "output_dim = len(dst_vocab)\n",
    "emb_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Инициализация модели\n",
    "model = Seq2Seq(input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dst_vocab['<pad>'])  # Игнорируем паддинговые токены\n",
    "\n",
    "# Пример: Проверим модель, чтобы убедиться, что она работает\n",
    "sample_src = torch.randint(0, input_dim, (32, 10)).to(device)  # Пример входных данных\n",
    "sample_dst = torch.randint(0, output_dim, (32, 10)).to(device)  # Пример целевых данных\n",
    "output = model(sample_src, sample_dst)\n",
    "print(output.shape)  # Проверим размерность выходных данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5337\n",
      "Epoch 2/10, Train Loss: 0.4457\n",
      "Epoch 3/10, Train Loss: 0.4109\n",
      "Epoch 4/10, Train Loss: 0.3851\n",
      "Epoch 5/10, Train Loss: 0.3638\n",
      "Epoch 6/10, Train Loss: 0.3454\n",
      "Epoch 7/10, Train Loss: 0.3294\n",
      "Epoch 8/10, Train Loss: 0.3159\n"
     ]
    }
   ],
   "source": [
    "# Функция обучения\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src_batch, dst_batch in train_loader:\n",
    "        src_batch, dst_batch = src_batch.to(device), dst_batch.to(device)\n",
    "\n",
    "        # Создание целевого индекса (сдвиг на 1, чтобы учить модель предсказывать следующий токен)\n",
    "        dst_input = dst_batch[:, :-1]\n",
    "        dst_target = dst_batch[:, 1:]\n",
    "\n",
    "        # Прямой проход\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_batch, dst_input)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        dst_target = dst_target.reshape(-1)\n",
    "        loss = criterion(output, dst_target)\n",
    "\n",
    "        # Обратный проход и обновление параметров\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация для вымышленного языка (разделение по символам)\n",
    "def tokenize_fictional(text):\n",
    "    # Разделяем строку на символы (каждый символ является токеном)\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация для английского языка\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_english(text):\n",
    "    # Используем spaCy для лемматизации и удаления стоп-слов и пунктуации\n",
    "    return [token.lemma_ for token in nlp(text) if not token.is_stop and not token.is_punct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Универсальная функция для токенизации в зависимости от языка\n",
    "def tokenize(text, language=\"english\"):\n",
    "    if language == \"english\":\n",
    "        return tokenize_english(text)\n",
    "    elif language == \"fictional\":\n",
    "        return tokenize_fictional(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация для каждого языка\n",
    "train_tokens = [\n",
    "    (tokenize(example['src'], 'fictional'), tokenize(example['dst'], 'english')) \n",
    "    for example in train_data\n",
    "]\n",
    "\n",
    "val_tokens = [\n",
    "    (tokenize(example['src'], 'fictional'), tokenize(example['dst'], 'english')) \n",
    "    for example in val_data\n",
    "]\n",
    "\n",
    "test_tokens = [\n",
    "    (tokenize(example['src'], 'fictional'), None)  # Поскольку нет перевода в test\n",
    "    for example in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['◄', '▴', '◓', '◠', '▨', ' ', '◨', '▽', '◠', '▦', '◈', '◬', '◓', '▪', '▼', '◬', '▵'], ['Intriguing'])\n"
     ]
    }
   ],
   "source": [
    "# Пример токенизированных данных\n",
    "print(train_tokens[0])  # Печать первого токенизированного примера из train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря для исходного языка (вымышленного языка) и целевого (английского)\n",
    "src_vocab = Counter([token for pair in train_tokens for token in pair[0]])\n",
    "dst_vocab = Counter([token for pair in train_tokens for token in pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортируем и создаем индексы для словаря\n",
    "src_vocab = {token: idx + 1 for idx, (token, _) in enumerate(src_vocab.most_common())}\n",
    "dst_vocab = {token: idx + 1 for idx, (token, _) in enumerate(dst_vocab.most_common())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем специальные токены\n",
    "src_vocab['<sos>'] = len(src_vocab) + 1  # Start of sentence\n",
    "src_vocab['<eos>'] = len(src_vocab) + 1  # End of sentence\n",
    "dst_vocab['<sos>'] = len(dst_vocab) + 1\n",
    "dst_vocab['<eos>'] = len(dst_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря для исходного языка (вымышленного): 178\n",
      "Размер словаря для целевого языка (английского): 53659\n",
      "Пример словаря (src): [(' ', 1), ('◠', 2), ('▦', 3), ('◓', 4), ('▱', 5), ('◎', 6), ('◪', 7), ('▴', 8), ('◗', 9), ('◫', 10)]\n",
      "Пример словаря (dst): [('know', 1), ('go', 2), ('right', 3), ('like', 4), ('think', 5), ('want', 6), ('okay', 7), ('get', 8), ('come', 9), ('tell', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Пример словаря\n",
    "print(f\"Размер словаря для исходного языка (вымышленного): {len(src_vocab)}\")\n",
    "print(f\"Размер словаря для целевого языка (английского): {len(dst_vocab)}\")\n",
    "\n",
    "# Пример добавления токенов в словарь\n",
    "print(f\"Пример словаря (src): {list(src_vocab.items())[:10]}\")  # Печать первых 10 слов\n",
    "print(f\"Пример словаря (dst): {list(dst_vocab.items())[:10]}\")  # Печать первых 10 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текста в индексы\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример индексов для первого примера из train: ([54, 8, 4, 2, 12, 1, 21, 19, 2, 3, 11, 16, 4, 18, 30, 16, 13], [24541])\n"
     ]
    }
   ],
   "source": [
    "# Добавляем '<unk>' в словарь, если его нет\n",
    "if '<unk>' not in src_vocab:\n",
    "    src_vocab['<unk>'] = len(src_vocab)\n",
    "\n",
    "if '<unk>' not in dst_vocab:\n",
    "    dst_vocab['<unk>'] = len(dst_vocab)\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
    "\n",
    "# Преобразуем все данные в последовательности индексов\n",
    "train_sequences = [(text_to_sequence(src, src_vocab), text_to_sequence(dst, dst_vocab)) for src, dst in train_tokens]\n",
    "val_sequences = [(text_to_sequence(src, src_vocab), text_to_sequence(dst, dst_vocab)) for src, dst in val_tokens]\n",
    "test_sequences = [(text_to_sequence(src, src_vocab), None) for src, _ in test_tokens]\n",
    "\n",
    "\n",
    "# Пример преобразования\n",
    "print(f\"Пример индексов для первого примера из train: {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка загруженных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "src_vocab[PAD_TOKEN] = len(src_vocab)\n",
    "dst_vocab[PAD_TOKEN] = len(dst_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Паддинг последовательностей\n",
    "def pad_sequences(batch):\n",
    "    src_seqs, dst_seqs = zip(*batch)  # Разделяем src и dst\n",
    "    src_seqs_padded = pad_sequence([torch.tensor(seq) for seq in src_seqs], padding_value=src_vocab[PAD_TOKEN])\n",
    "    dst_seqs_padded = pad_sequence([torch.tensor(seq) for seq in dst_seqs], padding_value=dst_vocab[PAD_TOKEN])\n",
    "    return src_seqs_padded, dst_seqs_padded\n",
    "\n",
    "# Создание кастомного collate_fn для DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_seqs_padded, dst_seqs_padded = pad_sequences(batch)\n",
    "    return src_seqs_padded, dst_seqs_padded\n",
    "\n",
    "# Создание DataLoader с кастомным collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_padded_sequences(output, target, pad_token_idx):\n",
    "    # Маскируем паддинг\n",
    "    mask = target != pad_token_idx\n",
    "    output = output[mask]\n",
    "    target = target[mask]\n",
    "    return output, target\n",
    "\n",
    "# Модифицируем функцию потерь\n",
    "def compute_loss(output, target, pad_token_idx):\n",
    "    output = output.view(-1, output.shape[-1])\n",
    "    target = target.view(-1)\n",
    "    output, target = mask_padded_sequences(output, target, pad_token_idx)\n",
    "    loss = criterion(output, target)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        # Эмбеддинги\n",
    "        self.src_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.dst_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # LSTM энкодер\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        # LSTM декодер\n",
    "        self.decoder = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        # Линейный слой для выхода\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Дропаут\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, dst):\n",
    "        # Эмбеддим входные последовательности\n",
    "        src_emb = self.src_embedding(src)\n",
    "        dst_emb = self.dst_embedding(dst)\n",
    "\n",
    "        # Процессинг с помощью энкодера\n",
    "        encoder_output, (hidden, cell) = self.encoder(src_emb)\n",
    "\n",
    "        # Декодируем выход энкодера\n",
    "        decoder_output, _ = self.decoder(dst_emb, (hidden, cell))\n",
    "        \n",
    "        # Прогнозируем выход\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение потерь и оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры модели\n",
    "input_dim = len(src_vocab)\n",
    "output_dim = len(dst_vocab)\n",
    "emb_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Инициализация модели\n",
    "model = Seq2Seq(input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout)\n",
    "\n",
    "# Функция потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Оптимизатор\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, dst in train_loader:\n",
    "        src = src.to(device)\n",
    "        dst = dst.to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прогоняем через модель\n",
    "        output = model(src, dst)\n",
    "\n",
    "        # Рассчитываем потери\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        dst = dst.view(-1)\n",
    "\n",
    "        loss = criterion(output, dst)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновляем параметры\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [30] at entry 0 and [56] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [30] at entry 0 and [56] at entry 1"
     ]
    }
   ],
   "source": [
    "# Переносим модель на устройство (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Обучаем модель\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
